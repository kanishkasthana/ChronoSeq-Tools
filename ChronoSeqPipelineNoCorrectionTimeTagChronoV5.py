#Author Kanishk Asthana kasthana@eng.ucsd.edu
#CHANGE THESE BEFORE YOU RUN THE SCRIPT:
num_cores=20 #Number of Cores to Use for STAR alignment and correcting substitution errors.
starGenomeIndex="/stg1/data2/kanishk/mixedHumanMouse/starGenomeIndexPE100/" #Absolute Path for Genome Index generated by STAR
ref_seq="/stg1/data2/kanishk/mixedHumanMouse/hg19_mm10_transgenes.fasta" #Absolute Path for Reference Genome used for Alignment
refFlat="/stg1/data2/kanishk/mixedHumanMouse/hg19_mm10_transgenes.refFlat" #Absolute Path for RefFlat annotations file for your Genome
dictPath="/stg1/data2/kanishk/mixedHumanMouse/hg19_mm10_transgenes.dict" #Dictionary needed for mergeBamAlignment from Picard even if you don't see the path explicity mentioned.
picard_path="/stg1/data2/kanishk/picard.jar" #Absolute Path for Picard tools jar file. Version:2.23.9 used for the development of this Pipeline
dropseq_path="/stg1/data2/kanishk/Drop-seq_tools-2.4.0/" #Absolute Path for Dropseq tools directory. Version 2.4.0 for the development of this Pipeline
chronoseq_path="/stg1/data2/kanishk/chrono-seq-tools/" #Absolute Path for Chrono-seq tools directory with all the python scripts.

#This is local storage on the compute node on which the code is running. Usually and SSD and Generally much faster than running it on a magnetic hard drive on the cluster.
#If you have a lot of datasets you are running in parallel, its important to do use scratch to prevent a slow down or your script from crashing.
#Sometimes also /TMP or /Temp. Check you node to see what the name of this directory is. This drive is generally erased after your session/job.
scratch_directory="/scratch"
#Global Variable that removes Intermediate Files if you don't want them to be stored.
global remove_intermediates
remove_intermediates=True
#This will copy the needed files to Scratch and then do all the operations in Scratch. Then copy back the final files to the directory path provided.
#Scratch on a SLURM node is generally an SSD so I/O operations are much faster. Otherwise this can be a bottleneck for the pipeline especially for Large Datasets.
global copy_to_scratch
copy_to_scratch=True
#We need to prevent multiple reads and writes from the Storage server at the same time if we are using scratch
#Please execute StartLock.py in a separate terminal on the same different compute node or a different node accessible by the other nodes running this script as a job. Use either the hostname or IP address of the node running the StartLock.py script. This is especially useful if you want to run multiple jobs at the same time stored on the same storage server but you don't want too many multiple requests for I/O from the server at the same time. Once the data is copied to or from scratch the pipeline should run independently or other jobs/instances.
ip_or_hostname_for_remote_lock="compute-10"
##############################################################################################################
import subprocess
import shlex
import os
import sys
import argparse
import shutil
from datetime import datetime,timedelta
global lock
from multiprocessing.managers import BaseManager
from threading import Lock
class LockManager(BaseManager):
    pass 
LockManager.register("getLock")
manager = LockManager(address=(ip_or_hostname_for_remote_lock, 55442), authkey=b'Lock')

#For writing output to the stdout in realtime
def write(*something):
    print(*something)
    sys.stdout.flush()
    
#User Inputs needed:            
sample_directory_path=""
FastQFileR1=""
FastQFileR2=""
sample_name=""

def parse_file(input_filename):
    if not os.path.isfile(input_filename):
        raise argparse.ArgumentTypeError("File does not exist. Please use a valid file path.")
    return(input_filename)

def parse_directory(input_directory_path):
    if not os.path.isdir(input_directory_path):
        raise argparse.ArgumentTypeError("This directory doesn't exit. Try another path or please create this directory before running again.")
    if input_directory_path[-1] == "/":
        return(input_directory_path[0:-1])
    else:
        return(input_directory_path)

parser=argparse.ArgumentParser(description="Script for Chrono-Seq Pipeline.")
parser.add_argument("READ1_FASTQ",help="Absolute path of .fastq.gz file for Read 1 of your Sample/Experiment", type=parse_file)
parser.add_argument("READ2_FASTQ",help="Absolute path of .fastq.gz file for Read 2 of your Sample/Experiment",type=parse_file)
parser.add_argument("SAMPLE_DIRECTORY_PATH",help="Please enter a Valid Absolute Path for Directory where all processed files will be stored. One directory for one sample please.",type=parse_directory)
parser.add_argument("SAMPLE_NAME",help="This string will be prefixed to most of the output files.",type=str)
args=parser.parse_args()
write(args)

sample_directory_path=args.SAMPLE_DIRECTORY_PATH
FastQFileR1=args.READ1_FASTQ
FastQFileR2=args.READ2_FASTQ
sample_name=args.SAMPLE_NAME

#Storing Original Values for Later Use
original_sample_directory_path=sample_directory_path
original_FastQFileR1=FastQFileR1
original_FastQFileR2=FastQFileR2
original_starGenomeIndex=starGenomeIndex
original_refSeq=ref_seq
original_refFlat=refFlat
original_dictPath=dictPath

#Creates fresh empty directory
def make_dir(tmp_directory_path):
    if os.path.isdir(tmp_directory_path):
        shutil.rmtree(tmp_directory_path)
        os.mkdir(tmp_directory_path)
    else:
        os.mkdir(tmp_directory_path)
        
#Copy a File to a New Directory Location and then get the latest Path
def copy_to_new_location(original_path,new_directory):
    fileName=os.path.basename(original_path)
    new_path=new_directory+"/"+fileName
    start_time=datetime.now()
    write("Started Copying : ",fileName," at ",start_time)
    shutil.copy(original_path,new_path) #Copy file
    end_time=datetime.now()
    time_taken=end_time-start_time
    write("Finished Copying : ",fileName," at ",end_time)
    write("Time Taken: ",time_taken)
    return new_path

def copy_directory_tree(original_directory,new_directory):
    start_time=datetime.now()
    write("Started Copying files to ",new_directory," at: ",start_time)
    shutil.copytree(original_directory,new_directory,dirs_exist_ok=True)
    end_time=datetime.now()
    write("Finished Copying files to ",new_directory," at: ",end_time)
    time_taken=end_time-start_time
    write("Time Taken :",time_taken)
    return new_directory
    
if copy_to_scratch:
    manager.connect()
    lock=manager.getLock()
    sample_directory_path=scratch_directory+"/"+sample_name
    make_dir(sample_directory_path)
    temporaryStarGenomeIndex=sample_directory_path+"/starGenomeIndex"
    make_dir(temporaryStarGenomeIndex)
    #Copying Files to newly created directories and getting their new name
    if lock.acquire():
        write("Lock Acquired Starting Copy to Scratch!")
        starGenomeIndex=copy_directory_tree(starGenomeIndex,temporaryStarGenomeIndex)
        FastQFileR1=copy_to_new_location(FastQFileR1,sample_directory_path)
        FastQFileR2=copy_to_new_location(FastQFileR2,sample_directory_path)
        ref_seq=copy_to_new_location(ref_seq,sample_directory_path)
        refFlat=copy_to_new_location(refFlat,sample_directory_path)
        dictPath=copy_to_new_location(dictPath,sample_directory_path)
        lock.release()

def unconditionally_remove_file(filename):
    os.remove(filename)
    write("Deleted File: "+filename)

def remove_file(filename):
    if remove_intermediates:
        os.remove(filename)
        write("Deleted file: "+filename)
        
def remove_directory_tree(tmp_directory_path):
    if os.path.isdir(tmp_directory_path):
        shutil.rmtree(tmp_directory_path)
        write("Deleted Directory: "+tmp_directory_path)
    
#This function will run a line directly on the command line and will wait for it to execute before going 
#to the next line
def bash(command):
    process = subprocess.Popen(shlex.split(command), stdout=subprocess.PIPE)
    while True:
        out = process.stdout.readline().decode()
        if out == '' and process.poll() is not None:
            break
        if out:
            write(out.strip())


#Making temporary Directory path. Also this will be removed at end of script if it still exists.
temporary_directory_path=sample_directory_path+"/TMP/"            
make_dir(temporary_directory_path)

def getNewFileName(previousFileName,additional_tag):
    newFileName=previousFileName.split('.')[0:-1]
    newFileName=".".join(newFileName)+additional_tag
    return(newFileName,previousFileName)

##############################################################################################################
#Merging Both Reads into a Single File:
OutputFileName=sample_directory_path+"/"+sample_name+"_unaligned.bam"
bash("java -Xmx4g -jar "+picard_path+" FastqToSam F1="+FastQFileR1+" F2="+FastQFileR2+ " O="+OutputFileName+" SM="+sample_name+" TMP_DIR="+temporary_directory_path)
if copy_to_scratch and FastQFileR1!=original_FastQFileR1 and FastQFileR2!=original_FastQFileR2:
    unconditionally_remove_file(FastQFileR1)
    unconditionally_remove_file(FastQFileR2)

#Creating New Files with Tagged Read1 Values

#Cell Tag
OutputFileName,previousFileName=getNewFileName(OutputFileName,".tagged.Cell.bam")
summaryFileName=OutputFileName+".summary.txt"
bash(dropseq_path+"/TagBamWithReadSequenceExtended INPUT="+previousFileName+" OUTPUT="+OutputFileName+" SUMMARY="+summaryFileName+" BASE_RANGE=1-12 BASE_QUALITY=10 BARCODED_READ=1 DISCARD_READ=False TAG_NAME=XC NUM_BASES_BELOW_QUALITY=1"+" TMP_DIR="+temporary_directory_path)
remove_file(previousFileName)

#Molecular Tag
OutputFileName,previousFileName=getNewFileName(OutputFileName,".Molecular.bam")
summaryFileName=OutputFileName+".summary.txt"
bash(dropseq_path+"/TagBamWithReadSequenceExtended INPUT="+previousFileName+" OUTPUT="+OutputFileName+" SUMMARY="+summaryFileName+" BASE_RANGE=13-26 BASE_QUALITY=10 BARCODED_READ=1 DISCARD_READ=False TAG_NAME=XM NUM_BASES_BELOW_QUALITY=1"+" TMP_DIR="+temporary_directory_path)
remove_file(previousFileName)

#Time Tag
OutputFileName,previousFileName=getNewFileName(OutputFileName,".Time.bam")
summaryFileName=OutputFileName+".summary.txt"
bash(dropseq_path+"/TagBamWithReadSequenceExtended INPUT="+previousFileName+" OUTPUT="+OutputFileName+" SUMMARY="+summaryFileName+" BASE_RANGE=27-86 BASE_QUALITY=10 BARCODED_READ=1 DISCARD_READ=True TAG_NAME=YT NUM_BASES_BELOW_QUALITY=150"+" TMP_DIR="+temporary_directory_path)
remove_file(previousFileName)

#Filter Low Quality Reads
OutputFileName,previousFileName=getNewFileName(OutputFileName,".Filtered.bam")
bash(dropseq_path+"/FilterBam TAG_REJECT=XQ INPUT="+previousFileName+" OUTPUT="+OutputFileName+" TMP_DIR="+temporary_directory_path)
remove_file(previousFileName)


#Trim Starting Sequence
OutputFileName,previousFileName=getNewFileName(OutputFileName,".Adapter_Trimmed.bam")
summaryFileName=OutputFileName+".adapter_trimming_report.txt"
bash(dropseq_path+"/TrimStartingSequence INPUT="+previousFileName+" OUTPUT="+OutputFileName+" OUTPUT_SUMMARY="+summaryFileName+" SEQUENCE=AAGCAGTGGTATCAACGCAGAGTGAATGGG  MISMATCHES=0  NUM_BASES=5"+" TMP_DIR="+temporary_directory_path)
remove_file(previousFileName)

#Trim PolyA Sequence
OutputFileName,previousFileName=getNewFileName(OutputFileName,".PolyA_Trimmed.bam")
summaryFileName=OutputFileName+".polyA_trimming_report.txt"
bash(dropseq_path+"/PolyATrimmer INPUT="+previousFileName+" OUTPUT="+OutputFileName+" OUTPUT_SUMMARY="+summaryFileName+" MISMATCHES=0  NUM_BASES=6"+" TMP_DIR="+temporary_directory_path)
remove_file(previousFileName)

#Preparing FastQ file for Alignment. STAR doesn't accept BAM files.
OutputFileName,previousFileName=getNewFileName(OutputFileName,".fastq")
bash("java -Xmx4g -jar "+picard_path+" SamToFastq INPUT="+previousFileName+" FASTQ="+OutputFileName+" TMP_DIR="+temporary_directory_path)
unaligned_bam=previousFileName

#Aligning Reads using STAR aligner:
outputFilePrefix=sample_directory_path+"/star"
bash("STAR --runThreadN "+str(num_cores)+" --genomeDir "+starGenomeIndex+" --readFilesIn "+ OutputFileName+" --outFileNamePrefix "+outputFilePrefix)
remove_file(OutputFileName)

#Sorting Aligned File
previousFileName=sample_directory_path+"/starAligned.out.sam"
OutputFileName=sample_directory_path+"/"+sample_name+".aligned.sorted.bam"
bash("java -Xmx4g -jar "+picard_path+" SortSam I="+previousFileName+" O="+OutputFileName+" SO=queryname"+" TMP_DIR="+temporary_directory_path)
remove_file(previousFileName)

#Merging with unaligned BAM file to get back Tags
OutputFileName,previousFileName=getNewFileName(OutputFileName,".merged.bam")
bash("java -Xmx4g -jar "+picard_path+" MergeBamAlignment REFERENCE_SEQUENCE="+ref_seq+" UNMAPPED_BAM="+unaligned_bam+" ALIGNED_BAM="+previousFileName+" OUTPUT="+OutputFileName+" INCLUDE_SECONDARY_ALIGNMENTS=false  PAIRED_RUN=false"+" TMP_DIR="+temporary_directory_path)
remove_file(previousFileName)
remove_file(unaligned_bam)

#Tagging Reads with Gene Functional information
OutputFileName,previousFileName=getNewFileName(OutputFileName,".tagged.bam")
bash(dropseq_path+"/TagReadWithGeneFunction INPUT="+previousFileName+" OUTPUT="+OutputFileName+" ANNOTATIONS_FILE="+refFlat+" TMP_DIR="+temporary_directory_path)
#remove_file(previousFileName)

#Correcting Substitution Errors
#OutputFileName,previousFileName=getNewFileName(OutputFileName,".sub_corrected.bam")
#bash("python "+chronoseq_path+"/CorrectSubstitutionErrorsMulticore.py "+previousFileName+" "+OutputFileName+" "+str(num_cores))
#remove_file(previousFileName)

#Correcting Synthesis Errors
#OutputFileName,previousFileName=getNewFileName(OutputFileName,".syn_corrected.bam")
#bash("python "+chronoseq_path+"/CorrectSynthesisErrors.py "+previousFileName+" "+OutputFileName)
#remove_file(previousFileName)

#Making Counts Vector for Barcodes
OutputFileName,previousFileName=getNewFileName(OutputFileName,".counts.txt.gz")
bash("python "+chronoseq_path+"/getBarcodeCounts.py "+previousFileName+" "+OutputFileName)

#Getting Time-Tags for the Barcodes
OutputFileName,previousFileName=getNewFileName(previousFileName,".time_tags.csv")
bash("python "+chronoseq_path+"/GetTimeTags.py "+previousFileName+" "+OutputFileName)
##############################################################################################################
#Getting Rid of temporary directory 
remove_directory_tree(temporary_directory_path)

if copy_to_scratch and refFlat!=original_refFlat:
    unconditionally_remove_file(refFlat)
if copy_to_scratch and ref_seq!=original_refSeq:
    unconditionally_remove_file(ref_seq)
if copy_to_scratch and dictPath!=original_dictPath:
    unconditionally_remove_file(dictPath)
if copy_to_scratch and starGenomeIndex!=original_starGenomeIndex:
    remove_directory_tree(starGenomeIndex)
    
#Copying all files still left in the scratch directory
if copy_to_scratch and lock.acquire() and original_sample_directory_path!=sample_directory_path:
    write("Lock Acquired Copying Final Files Back to Storage Server!")
    copy_directory_tree(sample_directory_path,original_sample_directory_path)
    remove_directory_tree(sample_directory_path) #Clean up scratch not automatically removed on our server.
    lock.release()